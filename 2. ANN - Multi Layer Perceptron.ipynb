{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Continuing from Single Layer Perceptron (Part 1)...\nCan we map our inputs to output for the following data ?\n\n| No. | Input | Output |\n| --- | ----- | --- |\n| 1 | 0 0 1 | 0 |\n| 2 | 1 1 0 | 0 |\n| 3 | 1 0 1 | 1 |\n| 4 | 0 1 1 | 1 |\n\n<br>\n- If we see very carefully, there is no direct correlation of any input column with the output\n- But, there is a correlation of combination of first two input columns with the output\n- For first two columns, if either of them is 1, then our output is 1\n- Here, we will have to make our ANN learn this pattern\n- To make this happen, we will have to assign different weights to every element of each input, so that every element will contribute to pattern identification\n- Not only this, we will also introduce one hidden layer this time\n- This hidden layer will aid us to deal with the complexity of the problem\n- The weights between Input layer and Hidden layer will map patterns among input elements\n- While the weights between Hidden layer and Output layer will decide on the final output depending on pattern "},{"metadata":{},"cell_type":"markdown","source":"# Let's get started"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"input = [[0,0,1],[0,1,1],[1,0,1],[1,1,0]]\noutput = [0, 1, 1, 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = np.array(input)\nprint(X.shape)\nX","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y = np.array(output).reshape(4,1)\nprint(Y.shape)\nY","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# seed random numbers to make calculation\n# deterministic (just a good practice)\nnp.random.seed(42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sigmoid(x):\n    return 1/(1+np.exp(-x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sigmoid_derivative(output):\n    return output * (1 - output)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# randomly initialize our weights \nsynapse0 = 2 * np.random.random((3,4)) - 1 # weights between Input layer and Hidden layer\nsynapse0.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, 3 represents the weights for each element of an input, while 4 represents the number of input samples."},{"metadata":{"trusted":true},"cell_type":"code","source":"synapse1 = 2 * np.random.random((4,1)) - 1 # weights between Hidden layer and Output layer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, 4 represents the weights for number of combinations produced at Hidden layer."},{"metadata":{"trusted":true},"cell_type":"code","source":"for j in range(50000):\n\n    # Feed forward\n    l0 = X                             # Input layer - 4 x 3 matrix\n    l1 = sigmoid(np.dot(l0,synapse0))  # Hidden layer - 4 x 3 dot 3 x 4 = 4 X 4 matrix\n    l2 = sigmoid(np.dot(l1,synapse1))  # Output layer - 4 x 4 dot 4 x 1 = 4 X 1 matrix\n    \n    # how much did we miss the target value?\n    l2_error = l2 - Y\n    \n    if(j % 10000 == 0):\n        print(\"Error: {}\".format(np.mean(np.abs(l2_error))))\n    \n    # error_weighted_derivative at Output layer\n    error_weighted_derivative2 = l2_error * sigmoid_derivative(l2)\n    synapse_derivative1 = np.dot(l1.T, error_weighted_derivative2)\n              \n    # how much did each l1 value contribute to the l2 error (according to the weights)?\n    l1_error = np.dot(error_weighted_derivative2, synapse1.T)\n    \n    # error_weighted_derivative at Hidden layer\n    error_weighted_derivative1 = l1_error * sigmoid_derivative(l1)\n    synapse_derivative0 = np.dot(l0.T, error_weighted_derivative1)\n\n    # learning !\n    synapse1 -= synapse_derivative1\n    synapse0 -= synapse_derivative0\n        \nprint(\"Output After Training:\")\nl2 ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# End notes...\n- Single Layer Perceptron made our way easy in understanding Multi Layer Perceptron\n- There are lot of questions still to be answered, we will take them up in upcoming posts\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}