{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:20%\">\n",
    "<tr>\n",
    "<th>No.</th>\n",
    "<th>Input</th>\n",
    "<th>Output</th>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>1</td>\n",
    "<td>0&nbsp;0&nbsp;1</td>\n",
    "<td>0</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>2</td>\n",
    "<td>1&nbsp;1&nbsp;1</td>\n",
    "<td>1</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>3</td>\n",
    "<td>1&nbsp;0&nbsp;1</td>\n",
    "<td>1</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>4</td>\n",
    "<td>0&nbsp;1&nbsp;1</td>\n",
    "<td>0</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "This is a perfect classification example for beginners.<br>\n",
    "Can we visually try to map the input to its corresponding output ?<br>\n",
    "If we did so, we would see that the leftmost input column is perfectly correlated with the output<br>\n",
    "This is our goal. Let's make Neural Network do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are Artificial Neural Networks(ANN) ?\n",
    "At its core, an ANN can be described by the following image.\n",
    "\n",
    "<img src=\"images/introduction-to-Artificial-Neural-Networks.jpg\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamentals\n",
    "- As the name suggests ANN is a network of interconnected perceptrons(represented by colourful circles)\n",
    "- Perceptrons are nodes where decisions are made (they draw parallels with neurons in the brain)\n",
    "- These decisions are made by feeding the input to the node to a function\n",
    "- A function can be linear(like Rectified Linear Unit - ReLU) or non-linear(like sigmoid)\n",
    "- On a lighter note, these function's name and their understandability are inversely proportional\n",
    "- The connection between two nodes draw parallels with synapses in the brain (they carry information between nodes)\n",
    "- In ANN, these synapses carry weights (information of degree of importance) of the input that is going to be feed to a node\n",
    "- A basic ANN has layers of nodes stacked in parallel (The number of layers depend on the complexity of the problem)\n",
    "- The input layer makes no decision, it just provides the input to the network\n",
    "- An ANN can have zero or n number of hidden layers, as said before - depending on the complexity of the problem\n",
    "- The final decison is made by output layer, which gives out the network's decision\n",
    "- An ANN has two primary responsibilities, first making a decision (by a process called Feed-Forward) and learn from those decisions (by a process called Back-Propagation) to make better decisions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's get our hands dirty!\n",
    "Lets create the most basic ANN to solve our problem mentioned before.\n",
    "\n",
    "<img src=\"images/slp.jpg\">\n",
    "\n",
    "- One important thing here to node down here is we have no hidden layer.\n",
    "- We have not used the hidden layer at the very go because we not sure of the complexity of our problem\n",
    "- The another important thing to note is, the input that is feed to the node at the output layer, is multiplied by its weight and products are summed\n",
    "- As mentioned before, a weight related to an input signifies the degree of importance of the same input\n",
    "- We will see later in this post, how to decide the function at the output node, which will determine the actual output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kickoff..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = [[0,0,1],[0,1,1],[1,0,1],[1,1,1]]\n",
    "output = [0, 0, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1],\n",
       "       [0, 1, 1],\n",
       "       [1, 0, 1],\n",
       "       [1, 1, 1]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(input)\n",
    "print(X.shape)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, 4 represents the number of samples of inputs, and 3 represents the number of elements in each input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = np.array(output).reshape(4,1)\n",
    "print(Y.shape)\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, 4 represents the number of outputs, and 1 represents the number of elements in each output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed random numbers to make calculation\n",
    "# deterministic (just a good practice)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.25091976],\n",
       "       [ 0.90142861],\n",
       "       [ 0.46398788]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We are initialize weights(synapses) randomly,\n",
    "#such that there is a mix of both positive and negative numbers between -1 and 1 (again a good practice)\n",
    "synapse = 2*np.random.random((3,1)) - 1\n",
    "print(synapse.shape)\n",
    "synapse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose the inputs are,<br>\n",
    "<table style=\"width:20%\">\n",
    "<tr>\n",
    "<th>I1</th>\n",
    "<th>I2</th>\n",
    "<th>I3</th>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>I11</td>\n",
    "<td>I12</td>\n",
    "<td>I13</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>I21</td>\n",
    "<td>I22</td>\n",
    "<td>I23</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>I31</td>\n",
    "<td>I32</td>\n",
    "<td>I33</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>I41</td>\n",
    "<td>I42</td>\n",
    "<td>I43</td>\n",
    "</tr>\n",
    "</table><br>\n",
    "\n",
    "and the weights are,<br>\n",
    "<table style=\"width:20%\">\n",
    "<tr>\n",
    "<th>W</th>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>W1</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>W2</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>W3</td>\n",
    "</tr>\n",
    "</table><br>\n",
    "Then, W1 represents weights for I1, W2 for I2 and W3 for I3.<br>\n",
    "This representation of data empowers us to use dot product from Linear algebra,<br>\n",
    "Because for a given input (I1, I2, I3), it is not only getting multiplied by its corresponding weight but also these products are summed.\n",
    "\n",
    "<table style=\"width:20%\">\n",
    "<tr>\n",
    "<th>Input for Output node</th>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>I11 x W1 + I12 x W2 + I13 x W3</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>I21 x W1 + I22 x W2 + I23 x W3</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>I31 x W1 + I32 x W2 + I33 x W3</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>I41 x W1 + I42 x W2 + I43 x W3</td>\n",
    "</tr>\n",
    "</table><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_for_output_node = np.dot(X, synapse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heading towards the goal post...<br>\n",
    "- Now we are looking for a function (which has to be non linear)that will take out input and return us the desired output in terms of 0 and 1.<br>\n",
    "- Here comes our saviour, [The Sigmoid function.](https://en.wikipedia.org/wiki/Sigmoid_function)<br>\n",
    "- As per wiki,<br>\n",
    "A sigmoid function is a<br>\n",
    "&nbsp;- bounded (between 0 and 1),<br>\n",
    "&nbsp;- differentiable,<br> \n",
    "&nbsp;- real function<br>\n",
    "that is defined for all real input values and has a <b>non-negative derivative</b> at each point.<br>\n",
    "- Another beautify of Sigmoid function is its derivative,<br>\n",
    "&nbsp;- at any given point \"x\" on the sigmoid curve, its dervivative is given by x * (1 - x)<br>\n",
    "&nbsp;- In a very short time we will see why this is help full\n",
    "- This what we wanted, lets feed our \"input_for_output_node\" in the sigmoid function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative(output):\n",
    "    return output * (1 - output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.61395979],\n",
       "       [0.79663861],\n",
       "       [0.55306642],\n",
       "       [0.75296649]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = sigmoid(input_for_output_node)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# and missed !\n",
    "- This is not what we expected.<br>\n",
    "- We expected output in either 0s or 1s.<br>\n",
    "- But what we got is decimal numbers that too some are close to 0.5<br>\n",
    "- This makes the interpretation of the output even more confusing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Light at end of the tunnel...\n",
    "- Remember, we had mentioned earlier that an ANN has two fundamental responsibilities<br>\n",
    "- First, Feed-Forward (making a decision), this what we have done till now<br>\n",
    "- We have used ANN to just make its first decision<br>\n",
    "- Feed-Forward - Passing the input data from input layer to output layer through various interactions through nodes<br>\n",
    "- What about the second fundamental duty ? Learning ?<br>\n",
    "- Back-Propagation - It's a process to identify various elements in the network which are reponsible for the error (in this case, its the weights) and its contribution to this error<br>\n",
    "- We don't only need to identify weights causing this error but also improve them<br>\n",
    "- This process of improving the weights causing error is called Optimization and we will use one(Gradient descent) in this post\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's bounce back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.61395979],\n",
       "       [ 0.79663861],\n",
       "       [-0.44693358],\n",
       "       [-0.24703351]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error = output - Y \n",
    "error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Catch !\n",
    "\n",
    "<img src=\"images/sigmoid-deriv-2.png\">\n",
    "\n",
    "- If the output represents the above three dots, the derivate of at these points generates the slopes of the lines\n",
    "- Notice that very high values such as x=2.0 (green dot) and very low values such as x=-1.0 (purple dot) have rather shallow slopes. \n",
    "- The highest slope you can have is at x=0 (blue dot). This plays an important role. Also notice that all derivatives are between 0 and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hold your breath !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_weighted_derivative = sigmoid_derivative(output) * error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When we multiply the \"slopes\" by the error, we are reducing the error of high confidence predictions\n",
    "- If the slope was really shallow (close to 0), then the network either had a very high value, or a very low value\n",
    "- This means that the network was quite confident one way or the other\n",
    "- However, if the network guessed something close to (x=0, y=0.5) then it isn't very confident\n",
    "- We update these \"wishy-washy\" predictions most heavily, and we tend to leave the confident ones alone by multiplying them by a number close to 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oversimplified Gradient Descent...\n",
    "Let's have a look at following sample neural network error curves,\n",
    "\n",
    "<img src=\"images/sgd_optimal.png\">\n",
    "\n",
    "- Calculate slope at current position\n",
    "- If slope is negative, move right\n",
    "- If slope is positive, move left\n",
    "- Repeat until slope == 0, when there is no error<br>\n",
    "<br>\n",
    "Gosh...Can Gradient Descent be simpler than this ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.15642501],\n",
       "       [ 0.08310967],\n",
       "       [ 0.11815141]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synapse_derivative = np.dot(X.T,error_weighted_derivative)\n",
    "synapse_derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "synapse -= synapse_derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This completes Back Propagation...first steps towards learning !\n",
    " - Now we have to Feed-Forward the the data inputs again with with updated synapse.\n",
    " - This process needs to be iterated until our error becomes close to zero\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error - [[ 0.58560757]\n",
      " [ 0.76208696]\n",
      " [-0.43749328]\n",
      " [-0.25546762]]\n",
      "Error - [[ 0.03187032]\n",
      " [ 0.02576904]\n",
      " [-0.0209258 ]\n",
      " [-0.02591085]]\n",
      "Error - [[ 0.02213675]\n",
      " [ 0.01794081]\n",
      " [-0.01459195]\n",
      " [-0.01801904]]\n",
      "Error - [[ 0.01793195]\n",
      " [ 0.01455045]\n",
      " [-0.01184284]\n",
      " [-0.01460442]]\n",
      "Error - [[ 0.01545647]\n",
      " [ 0.01255128]\n",
      " [-0.01022002]\n",
      " [-0.01259248]]\n",
      "Output After Training:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.01378176],\n",
       "       [0.01119732],\n",
       "       [0.99087981],\n",
       "       [0.98876935]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for iter in range(5000):\n",
    "\n",
    "    # forward propagation\n",
    "    layer_0 = X\n",
    "    layer_1 = sigmoid(np.dot(layer_0,synapse))\n",
    "\n",
    "    # how much did we miss?\n",
    "    layer_1_error = layer_1 - Y\n",
    "    \n",
    "    if(iter % 1000 == 0):\n",
    "        print(\"Error - {}\".format(layer_1_error))\n",
    "\n",
    "    # multiply how much we missed by the \n",
    "    # slope of the sigmoid at the values in layer_1\n",
    "    layer_1_delta = layer_1_error * sigmoid_derivative(layer_1)\n",
    "    synapse_0_derivative = np.dot(layer_0.T,layer_1_delta)\n",
    "\n",
    "    # update weights\n",
    "    synapse -= synapse_0_derivative\n",
    "    \n",
    "print(\"Output After Training:\")\n",
    "layer_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End notes<br>\n",
    " - This is the basic NN architecture\n",
    " - What if our errors start to increase again ? Can this happen ? \n",
    " - Can our implementation skip the zero level error and pass through other end of error curve ?\n",
    " - Can we tune the number of iterations ?\n",
    " - These are some of the important questions I will be answering in my future post.<br>\n",
    " \n",
    " Till that time can you solve the below problem ?<br>\n",
    "\n",
    "<table style=\"width:20%\">\n",
    "<tr>\n",
    "<th>No.</th>\n",
    "<th>Input</th>\n",
    "<th>Output</th>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>1</td>\n",
    "<td>0&nbsp;0&nbsp;1</td>\n",
    "<td>0</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>2</td>\n",
    "<td>1&nbsp;1&nbsp;0</td>\n",
    "<td>0</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>3</td>\n",
    "<td>1&nbsp;0&nbsp;1</td>\n",
    "<td>1</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>4</td>\n",
    "<td>0&nbsp;1&nbsp;1</td>\n",
    "<td>1</td>\n",
    "</tr>\n",
    "</table>\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
